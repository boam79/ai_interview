/**
 * Audio Capture Utilities
 * 
 * Web Audio API utilities for:
 * - Microphone access & permissions
 * - Real-time audio stream capture
 * - Audio analysis (frequency/amplitude data)
 * - Resource cleanup
 */

export interface AudioCaptureState {
  stream: MediaStream | null;
  audioContext: AudioContext | null;
  analyser: AnalyserNode | null;
  microphone: MediaStreamAudioSourceNode | null;
  dataArray: Uint8Array | null;
}

/**
 * Get the best available microphone with detailed info (prioritize Bluetooth)
 * @returns Best microphone device info
 */
export async function getBestMicrophoneInfo(): Promise<{
  deviceId: string;
  label: string;
  isBluetooth: boolean;
}> {
  try {
    console.log('ğŸ” [Audio] Finding best microphone...');

    // Request permission first to get device labels
    await navigator.mediaDevices.getUserMedia({ audio: true });

    const devices = await navigator.mediaDevices.enumerateDevices();
    const audioInputs = devices.filter(device => device.kind === 'audioinput');
    
    console.log('ğŸ¤ [Audio] Available microphones:', audioInputs.length);
    console.log('ğŸ¤ [Audio] Device details:', audioInputs.map(d => ({
      deviceId: d.deviceId,
      label: d.label,
      isBluetooth: isBluetoothDevice(d)
    })));

    // Prioritize Bluetooth devices
    const bluetoothDevices = audioInputs.filter(isBluetoothDevice);
    
    if (bluetoothDevices.length > 0) {
      const selectedDevice = bluetoothDevices[0];
      console.log('ğŸ§ [Audio] Bluetooth device selected:', selectedDevice.label);
      return {
        deviceId: selectedDevice.deviceId,
        label: selectedDevice.label || 'Unknown Bluetooth Device',
        isBluetooth: true
      };
    }

    // If no Bluetooth, use first available device
    if (audioInputs.length > 0) {
      const selectedDevice = audioInputs[0];
      console.log('ğŸ¤ [Audio] Default device selected:', selectedDevice.label);
      return {
        deviceId: selectedDevice.deviceId,
        label: selectedDevice.label || 'Default Microphone',
        isBluetooth: false
      };
    }

    console.log('ğŸ¤ [Audio] No specific device found, using default');
    return {
      deviceId: 'default',
      label: 'Default Microphone',
      isBluetooth: false
    };

  } catch (error) {
    console.warn('âš ï¸ [Audio] Could not enumerate devices, using default:', error);
    return {
      deviceId: 'default',
      label: 'Default Microphone',
      isBluetooth: false
    };
  }
}

/**
 * Get the best available microphone (prioritize Bluetooth)
 * @returns Best microphone device ID or 'default'
 */
export async function getBestMicrophone(): Promise<string> {
  const info = await getBestMicrophoneInfo();
  return info.deviceId;
}

/**
 * Request microphone access from user (auto-select best microphone including Bluetooth)
 * @returns Object with MediaStream and microphone info
 */
export async function requestMicrophoneAccess(): Promise<{
  stream: MediaStream;
  microphoneInfo: {
    deviceId: string;
    label: string;
    isBluetooth: boolean;
  };
}> {
  try {
    console.log('ğŸ¤ [Audio] Requesting microphone access...');

    // Check if browser supports getUserMedia
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      throw new Error('Your browser does not support microphone access');
    }

    // Get the best microphone automatically with detailed info
    const microphoneInfo = await getBestMicrophoneInfo();

    // Request microphone permission with Bluetooth-friendly settings
    const constraints = {
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
        sampleRate: 44100,
        channelCount: 1, // Mono for better compatibility
        latency: 0.1, // Lower latency for better real-time experience
        // Use the best device we found
        ...(microphoneInfo.deviceId !== 'default' && { deviceId: { ideal: microphoneInfo.deviceId } }),
      },
    };

    console.log('ğŸ§ [Audio] Using constraints:', constraints);
    const stream = await navigator.mediaDevices.getUserMedia(constraints);

    // Log the audio tracks for debugging
    const audioTracks = stream.getAudioTracks();
    audioTracks.forEach((track, index) => {
      console.log(`ğŸµ [Audio] Track ${index}:`, {
        label: track.label,
        enabled: track.enabled,
        muted: track.muted,
        readyState: track.readyState,
        settings: track.getSettings()
      });
    });

    console.log('âœ… [Audio] Microphone access granted');
    console.log('ğŸ¤ [Audio] Selected microphone:', microphoneInfo);
    
    return { stream, microphoneInfo };

  } catch (error: any) {
    console.error('âŒ [Audio] Microphone access error:', error);

    // Handle specific error types
    if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
      throw new Error('ë§ˆì´í¬ ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.');
    } else if (error.name === 'NotFoundError' || error.name === 'DevicesNotFoundError') {
      throw new Error('ë§ˆì´í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì´í¬ë‚˜ ë¸”ë£¨íˆ¬ìŠ¤ ì´ì–´í°ì´ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.');
    } else if (error.name === 'NotReadableError' || error.name === 'TrackStartError') {
      throw new Error('ë§ˆì´í¬ê°€ ë‹¤ë¥¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì•±ì„ ì¢…ë£Œí•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
    } else if (error.name === 'OverconstrainedError') {
      throw new Error('ë§ˆì´í¬ ì„¤ì •ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë¸”ë£¨íˆ¬ìŠ¤ ì´ì–´í°ì˜ ê²½ìš° í˜ì–´ë§ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.');
    } else {
      throw new Error(`ë§ˆì´í¬ ì˜¤ë¥˜: ${error.message}`);
    }
  }
}

/**
 * Start audio stream and set up real-time analysis
 * @param stream MediaStream from getUserMedia
 * @returns AudioCaptureState with all necessary objects
 */
export function startAudioStream(stream: MediaStream): AudioCaptureState {
  try {
    console.log('ğŸ”Š [Audio] Starting audio stream...');

    // Create AudioContext
    const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
    
    // Create AnalyserNode for real-time frequency/amplitude data
    const analyser = audioContext.createAnalyser();
    analyser.fftSize = 2048; // Higher = more detail, but slower
    analyser.smoothingTimeConstant = 0.8; // 0-1, higher = smoother

    // Create microphone source
    const microphone = audioContext.createMediaStreamSource(stream);
    
    // Connect microphone to analyser
    microphone.connect(analyser);

    // Create data array for frequency data
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);

    console.log('âœ… [Audio] Audio stream started', {
      fftSize: analyser.fftSize,
      bufferLength,
      sampleRate: audioContext.sampleRate,
    });

    return {
      stream,
      audioContext,
      analyser,
      microphone,
      dataArray,
    };

  } catch (error: any) {
    console.error('âŒ [Audio] Failed to start audio stream:', error);
    throw new Error(`Failed to start audio stream: ${error.message}`);
  }
}

/**
 * Stop audio stream and clean up all resources
 * @param state AudioCaptureState to clean up
 */
export function stopAudioStream(state: AudioCaptureState): void {
  try {
    console.log('ğŸ›‘ [Audio] Stopping audio stream...');

    // Stop all tracks in the stream
    if (state.stream) {
      state.stream.getTracks().forEach(track => {
        track.stop();
        console.log('  ğŸ”‡ [Audio] Track stopped:', track.kind);
      });
    }

    // Disconnect microphone
    if (state.microphone) {
      state.microphone.disconnect();
    }

    // Close audio context
    if (state.audioContext && state.audioContext.state !== 'closed') {
      state.audioContext.close();
    }

    console.log('âœ… [Audio] Audio stream stopped and cleaned up');

  } catch (error: any) {
    console.error('âŒ [Audio] Error stopping audio stream:', error);
  }
}

/**
 * Get real-time frequency data (for waveform visualization)
 * @param analyser AnalyserNode
 * @param dataArray Uint8Array buffer
 * @returns Updated dataArray with frequency data (0-255)
 */
export function getFrequencyData(
  analyser: AnalyserNode, 
  dataArray: Uint8Array
): Uint8Array {
  analyser.getByteFrequencyData(dataArray);
  return dataArray;
}

/**
 * Get real-time time domain data (for waveform visualization)
 * @param analyser AnalyserNode
 * @param dataArray Uint8Array buffer
 * @returns Updated dataArray with time domain data (0-255)
 */
export function getTimeDomainData(
  analyser: AnalyserNode,
  dataArray: Uint8Array
): Uint8Array {
  analyser.getByteTimeDomainData(dataArray);
  return dataArray;
}

/**
 * Calculate average amplitude from frequency data (for visual feedback)
 * @param dataArray Uint8Array with frequency data
 * @returns Average amplitude (0-255)
 */
export function getAverageAmplitude(dataArray: Uint8Array): number {
  const sum = dataArray.reduce((acc, value) => acc + value, 0);
  return sum / dataArray.length;
}

/**
 * Calculate normalized amplitude (0-1) for scaling effects
 * @param dataArray Uint8Array with frequency data
 * @returns Normalized amplitude (0-1)
 */
export function getNormalizedAmplitude(dataArray: Uint8Array): number {
  return getAverageAmplitude(dataArray) / 255;
}

/**
 * Check if browser supports Web Audio API
 * @returns true if supported, false otherwise
 */
export function isWebAudioSupported(): boolean {
  return !!(
    window.AudioContext || 
    (window as any).webkitAudioContext
  );
}

/**
 * Check if browser supports getUserMedia
 * @returns true if supported, false otherwise
 */
export function isMicrophoneSupported(): boolean {
  return !!(
    navigator.mediaDevices && 
    navigator.mediaDevices.getUserMedia
  );
}

/**
 * Request specific microphone device (including Bluetooth)
 * @param deviceId Specific device ID to use
 * @returns MediaStream if granted, throws error if denied
 */
export async function requestSpecificMicrophone(deviceId: string): Promise<MediaStream> {
  try {
    console.log('ğŸ§ [Audio] Requesting specific microphone:', deviceId);

    const stream = await navigator.mediaDevices.getUserMedia({
      audio: {
        deviceId: { exact: deviceId },
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
        sampleRate: 44100,
        channelCount: 1,
        latency: 0.1,
      },
    });

    console.log('âœ… [Audio] Specific microphone access granted');
    return stream;

  } catch (error: any) {
    console.error('âŒ [Audio] Specific microphone access error:', error);
    throw error;
  }
}

/**
 * Get list of available audio input devices
 * @returns Array of audio input devices
 */
export async function getAvailableMicrophones(): Promise<MediaDeviceInfo[]> {
  try {
    console.log('ğŸ” [Audio] Getting available microphones...');

    // Request permission first to get device labels
    await navigator.mediaDevices.getUserMedia({ audio: true });

    const devices = await navigator.mediaDevices.enumerateDevices();
    const audioInputs = devices.filter(device => device.kind === 'audioinput');

    console.log('ğŸ¤ [Audio] Available microphones:', audioInputs.length);
    return audioInputs;

  } catch (error: any) {
    console.error('âŒ [Audio] Error getting microphones:', error);
    return [];
  }
}

/**
 * Check if device is likely a Bluetooth device
 * @param device MediaDeviceInfo
 * @returns true if likely Bluetooth
 */
export function isBluetoothDevice(device: MediaDeviceInfo): boolean {
  const label = device.label.toLowerCase();
  const bluetoothKeywords = [
    'bluetooth',
    'bt',
    'airpods',
    'airpods pro',
    'airpods max',
    'galaxy buds',
    'galaxy buds pro',
    'galaxy buds live',
    'galaxy buds2',
    'wh-1000xm',
    'wh-1000xm4',
    'wh-1000xm5',
    'wf-1000xm',
    'bose',
    'quietcomfort',
    'soundsport',
    'sony',
    'wireless',
    'beats',
    'jabra',
    'sennheiser',
    'momentum',
    'pixel buds',
    'buds pro',
    'earbuds'
  ];
  
  return bluetoothKeywords.some(keyword => label.includes(keyword));
}

/**
 * Get user-friendly error message for microphone errors
 * @param error Error object
 * @returns User-friendly error message
 */
export function getMicrophoneErrorMessage(error: any): string {
  if (!isMicrophoneSupported()) {
    return 'ì£„ì†¡í•©ë‹ˆë‹¤. ì´ ë¸Œë¼ìš°ì €ëŠ” ë§ˆì´í¬ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìµœì‹  Chrome, Safari, ë˜ëŠ” Edgeë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.';
  }

  if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
    return 'ë§ˆì´í¬ ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.';
  }

  if (error.name === 'NotFoundError' || error.name === 'DevicesNotFoundError') {
    return 'ë§ˆì´í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì´í¬ë‚˜ ë¸”ë£¨íˆ¬ìŠ¤ ì´ì–´í°ì´ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.';
  }

  if (error.name === 'NotReadableError' || error.name === 'TrackStartError') {
    return 'ë§ˆì´í¬ê°€ ë‹¤ë¥¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì•±ì„ ì¢…ë£Œí•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.';
  }

  if (error.name === 'OverconstrainedError') {
    return 'ë§ˆì´í¬ ì„¤ì •ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë¸”ë£¨íˆ¬ìŠ¤ ì´ì–´í°ì˜ ê²½ìš° í˜ì–´ë§ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.';
  }

  return `ë§ˆì´í¬ ì˜¤ë¥˜: ${error.message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'}`;
}

